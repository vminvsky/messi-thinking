import os
import json
from datasets import load_dataset
from tqdm import tqdm
from dynamic_scaling.prompt import SKY_T1_SYSTEM_PROMPT, SKY_T1_FIXED, BASE_MODEL_SYSTEM_PROMPT, QWEN_BASE_MODEL_PROMPT
from together import Together
from openai import AsyncOpenAI
import logging
from vllm import LLM, SamplingParams
from dynamic_scaling.prompt import generate_prompt
from concurrent.futures import ThreadPoolExecutor, as_completed
import asyncio

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Parameters
K = 50  # tokens for base model generation per turn
P = 300  # tokens for instruct model generation per turn
TOTAL_NEW_TOKENS = 8192
NUM_SAMPLES = 6  # samples per dataset entry
OUTPUT_DIR = "taco_medium_qwen_32b"

TENSOR_PARALLEL_SIZE = 2

USE_OPENAI = False  # Set to True to use OpenAI client for inference
USE_VLLM = True
USE_MIX = False

def get_prompt(sample):
    """Parse test cases and starter code from problem to create a prompt for the LLM."""
    test_case = json.loads(sample["input_output"])
    starter_code = sample["starter_code"]
    prompt_text = generate_prompt(test_case, sample["question"], starter_code)
    return [{"role": "system", "content": SKY_T1_FIXED}, {"role": "user", "content": prompt_text}]

os.makedirs(OUTPUT_DIR, exist_ok=True)

base_model = "Qwen/Qwen2.5-32B"
instruct_model = "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B"

if USE_OPENAI:
    base_client = AsyncOpenAI(
        api_key=os.environ.get("OPENAI_API_KEY"),
        base_url="https://api.openai.com/v1",
    )
    instruct_client = AsyncOpenAI(
        api_key=os.environ.get("OPENAI_API_KEY"),
        base_url="https://api.openai.com/v1",
    )
elif USE_VLLM:
    base_client = AsyncOpenAI(api_key="token-abc123", base_url="http://localhost:8051/v1")
    instruct_client = AsyncOpenAI(api_key="token-abc123", base_url="http://localhost:8000/v1")
elif USE_MIX:
    base_client = LLM(model=base_model, gpu_memory_utilization=0.8, trust_remote_code=True, tensor_parallel_size=TENSOR_PARALLEL_SIZE)
    instruct_client = Together(
        api_key=os.environ.get("TOGETHER_API_KEY"),
        base_url="https://api.together.xyz/v1",
    )

base_sampling_params = SamplingParams(temperature=0.6, top_p=0.7, top_k=50, max_tokens=K)
instruct_sampling_params = SamplingParams(temperature=0.6, top_p=0.7, top_k=50, max_tokens=P)

_ds = load_dataset("BAAI/TACO", trust_remote_code=True)["train"].filter(lambda x: x["difficulty"] == "MEDIUM")

async def perform_base_inference(input_text):
    if USE_OPENAI:
        completion = await asyncio.to_thread(
            base_client.completions.create,
            model=base_model,
            prompt=input_text,
            max_tokens=K,
            temperature=0.8,
            top_p=0.95
        )
        generated = completion.choices[0].text.replace("</think>", "[end_of_thought]").replace("<think>", "[begin_of_thought]")
        tokens_generated = len(generated.split())
    else:
        completion = await base_client.completions.create(
            model=base_model,
            prompt=input_text,
            max_tokens=K,
            temperature=0.8,
            top_p=0.95
        )
        generated = completion.choices[0].text.replace("</think>", "[end_of_thought]").replace("<think>", "[begin_of_thought]")
        tokens_generated = completion.usage.completion_tokens
    return generated, tokens_generated

async def perform_instruct_inference(conversation):
    chat_completion = await instruct_client.chat.completions.create(
        model=instruct_model,
        messages=conversation,
        max_tokens=P,
        temperature=0.7,
        top_p=0.7,
        extra_body={"continue_final_message": True, 
                "add_generation_prompt": False},
    )
    generated = chat_completion.choices[0].message.content.replace("</think>", "[end_of_thought]").replace("<think>", "[begin_of_thought]")
    tokens_generated = chat_completion.usage.completion_tokens
    return generated, tokens_generated

async def process_sample(idx, sample, sample_num, prompt, output_filename):
    current_text = ""
    total_generated_tokens = 0
    round_num = 0
    while total_generated_tokens < TOTAL_NEW_TOKENS:
        if round_num % 2 == 1:
            # input_text = BASE_MODEL_SYSTEM_PROMPT.format(Question=prompt[1]["content"]) + " Assistant: " + current_text
            input_text = QWEN_BASE_MODEL_PROMPT + prompt[1]["content"] + "<|im_end|>\n<|im_start|>assistant\n" + current_text
            generated, tokens_generated = await perform_base_inference(input_text)
            if tokens_generated < K:
                current_text += generated
                total_generated_tokens += tokens_generated
                logger.info("+++Less than K tokens generated by base model+++")
                break
            current_text += generated
            total_generated_tokens += tokens_generated
        else:
            conversation = [
                {"role": "system", "content": prompt[0]["content"]},
                {"role": "user", "content": prompt[1]["content"]},
                {"role": "assistant", "content": "[begin_of_thought]" + current_text}
            ]
            generated, tokens_generated = await perform_instruct_inference(conversation)
            if tokens_generated < P:
                current_text += generated
                total_generated_tokens += tokens_generated
                logger.info("+++Less than P tokens generated by instruction model+++")
                break
            current_text += generated
            total_generated_tokens += tokens_generated
        round_num += 1
        if total_generated_tokens >= TOTAL_NEW_TOKENS:
            logger.info("+++Total generated tokens reached TOTAL_NEW_TOKENS+++")
            break
    output_data = {
        "prompt": prompt,
        "generated_text": "[begin_of_thought]" + current_text,
        "metadata": sample,
        "question_id": idx,
        "sample_index": sample_num,
        "generation_config": {
            "base_model": base_model,
            "instruct_model": instruct_model,
            "base_temperature": base_sampling_params.temperature,
            "base_top_p": base_sampling_params.top_p,
            "base_top_k": base_sampling_params.top_k,
            "base_max_tokens": base_sampling_params.max_tokens,
            "instruct_temperature": instruct_sampling_params.temperature,
            "instruct_top_p": instruct_sampling_params.top_p,
            "instruct_top_k": instruct_sampling_params.top_k,
            "instruct_max_tokens": instruct_sampling_params.max_tokens
        }
    }
    with open(output_filename, "w") as f:
        json.dump(output_data, f, indent=2)

async def main():
    semaphore = asyncio.Semaphore(90)
    tasks = []
    for idx, sample in tqdm(enumerate(_ds), desc="Processing samples"):
        prompt = get_prompt(sample)
        if not prompt:
            continue
        for sample_num in range(NUM_SAMPLES):
            output_filename = os.path.join(OUTPUT_DIR, f"question_{idx}_sample_{sample_num}.json")
            if os.path.exists(output_filename):
                logger.info(f"Output file {output_filename} exists, skipping sample {sample_num} for question_{idx}")
                continue
            tasks.append(asyncio.create_task(limited_process_sample(semaphore, idx, sample, sample_num, prompt, output_filename)))
    for task in tqdm(asyncio.as_completed(tasks), total=len(tasks), desc="Waiting for tasks"):
        await task

async def limited_process_sample(semaphore, idx, sample, sample_num, prompt, output_filename):
    async with semaphore:
        await process_sample(idx, sample, sample_num, prompt, output_filename)

if __name__ == "__main__":
    asyncio.run(main()) 